---
layout: post
title: Classical Momentum and Nesterov’s Accelerated Gradient Descent
comments: True
---

冲量（Classical Momentum， 简写为 CM ）和 Nesterov 加速 （ Nesterov’s Accelerated Gradient (NAG) ） 在 SGD 中常用，他们的步骤相似。经典的冲量表示如下：

$$
\begin{align}
\Delta \theta_t &= \alpha \Delta \theta_{t-1} - \eta \nabla f(\theta_{t-1}) \\
\theta_t &= \theta_{t-1} + \Delta \theta_t
\end{align}
$$

Nesterov’s Accelerated Gradient (NAG) 的方法表示如下 [1]

$$
\begin{align}
y_{s+1} &= x_s - \eta \nabla f(x_s) \\
x_{s+1} &= (1+\alpha)y_{s+1} - \alpha y_s
\end{align}
$$

其中 $$ \eta $$ 是步长，在 NAG 中 $$ \eta = 1/\beta$$， $$\alpha = \frac{\sqrt{Q}-1}{\sqrt{Q}-1} $$， $$ Q$$ 是函数的条件数。NAG 的理解比较困难，Sébastien Bubeck 提供了一个从几何角度的解释 [2]。其实 NAG 可以表示为经典的冲量类似的形式 [3]。


我们定义 $$ \theta_s = y_s$$， 以及 $$ \Delta \theta_{s+1} = \theta_{s+1} - \theta_s$$， 从 NAG 的 $$ x_s$$ 的 updating rule 可以得到

$$
\begin{align}
x_{s+1} &= (1+\alpha) \theta_{s+1} - \alpha \theta_s \\
&= \theta_{s+1} + \alpha (\theta_{s+1} - \theta_s) \\
&= \theta_{s+1} + \alpha \Delta \theta_{s+1}
\end{align}
$$

因此有 $$x_{s} = \theta_{s} + \alpha \Delta \theta_{s}$$， 代入 NAG 的 $$ y_{s+1}$$ 迭代公式可以得到

$$
\begin{align}
 \theta_{s+1} &= \theta_{s} + \alpha \Delta \theta_{s} - \eta \Delta f(\theta_{s} + \alpha \Delta \theta_{s}) \\
 \Delta \theta_{s+1} &= \alpha \Delta \theta_{s} - \eta \Delta f(\theta_{s} + \alpha \Delta \theta_{s})
\end{align}
$$

因此 NAG 可以看作如下的更新过程

$$
\begin{align}
\Delta \theta_{s+1} &= \alpha \Delta \theta_{s} - \eta \Delta f(\theta_{s} + \alpha \Delta \theta_{s}) \\
\theta_{s+1} &=  \theta_s + \Delta \theta_{s+1} 
\end{align}
$$

从上面的变换可以看出 NAG 的梯度要比 CM 要提前一步。如果步长合适， NAG 比 CM 有优势，如果不合适就情况就不一定了。下图中用的步长是对于 NAG 在理想情况下的步长。代码见[这里](https://gist.github.com/cswhjiang/676b410a975b65761e8d)。从图中可以看出 NAG 确实比 CM 更稳定一点。其实如果步长选的不合适，有些情况下 CM 可能会比 NAG 稳定， 以为在求梯度的时候 NAG 比 CM 更激进一点，有的情况下会有产生不稳定的情况。
![nag和cm的比较](/figures/gd_cm_nag_my.png)


```language matlab
function test_nag()
r = randn(2,2);

a00 = 1;
a11 = 40;

A = [a00 0;
    0, a11];
% A = A + r*r';
b = [0; 0];


xs = -40:0.5:40;
ys = -40:0.5:40;
zs = zeros(length(xs), length(ys));
for i = 1:length(xs)
    for j = 1:length(ys)
        zs(j,i) = f([xs(i); ys(j)], A, b); %%%% NOTE: it should be zs(j,i), NOT zs(i,j)
    end
end

max_iter = 100;
% eta = 0.01*4;
% mu = 0.5;

eta = 1/a11/2;
Q = a11/a00;
mu = (sqrt(Q) - 1)/(sqrt(Q)+1);

x0 = [35;35];
[x_array_GD, obj_array_GD] = gradient_descent(x0, A, b, eta,max_iter);
[x_array_CM, obj_array_CM] = momentum(x0, A, b, eta, mu,max_iter);
[x_array_NAG, obj_array_NAG] = nag(x0, A, b, eta, mu,max_iter);
 
figure;
contour(xs,ys,zs,50);hold on;
h1 = plot(x_array_GD(1,:),x_array_GD(2,:),'-ks','MarkerFaceColor','r','MarkerSize',8); hold on;
h2 = plot(x_array_CM(1,:),x_array_CM(2,:),'-ko','MarkerFaceColor','g','MarkerSize',8);hold on;
h3 = plot(x_array_NAG(1,:),x_array_NAG(2,:),'-kd','MarkerFaceColor','b','MarkerSize',8);hold on;
h_legend = legend([h1 h2 h3], 'GD','CM','NAG','Location','SouthWest');
set(h_legend,'FontSize',24);
saveas(gcf,'gd_cm_nag_my.png');

figure;
plot(obj_array_GD, '-b');hold on;
plot(obj_array_CM, '--g');hold on;
plot(obj_array_NAG, ':r');hold on;
legend('GD','CM','NAG');
end

function y = f(x,A,b) % y = x'*A*x + b'*x
y = x'*A*x + x'*b;
end

function y = g(x,A,b) % return gradient
y = 2*A*x + b;
end


function [x_array, obj_array] = gradient_descent(x0, A, b, eta,max_iter)

x = x0;
x_array = [];
x_array = [x_array;x];
obj0 = f(x, A, b);
obj_array = [];
obj_array=[obj_array; obj0];
for i = 1:max_iter
    gradient = g(x,A,b);
    x = x - eta*gradient;
    x_array = [x_array x];
    obj = f(x, A, b);
    obj_array=[obj_array; obj];
%     assert(obj < obj0);
    if obj0 - obj < obj * 1e-4;
       break; 
    end
end
end

function [x_array, obj_array] = momentum(x0, A, b, eta, mu,max_iter)
x = x0;
x_array = [];
x_array = [x_array;x];
obj0 = f(x, A, b);
obj_array = [];
obj_array=[obj_array; obj0];

v = zeros(size(x));
for i = 1:max_iter
    gradient = g(x,A,b);
    v = mu*v - eta*gradient;
    x = x + v;
    x_array = [x_array x];
    obj = f(x, A, b);
    obj_array=[obj_array; obj];
%     assert(obj < obj0);
    if obj0 - obj < obj * 1e-4;
       break; 
    end
end
end

function [x_array, obj_array] = nag(x0, A, b, eta, mu,max_iter)
x = x0;
x_array = [];
x_array = [x_array;x];
obj0 = f(x, A, b);
obj_array = [];
obj_array=[obj_array; obj0];

v = zeros(size(x));
for i = 1:max_iter
    gradient = g(x + mu*v,A,b);
    v = mu*v - eta*gradient;
    x = x + v;
    x_array = [x_array x];
    obj = f(x, A, b);
    obj_array=[obj_array; obj];
%     assert(obj < obj0);
    if obj0 - obj < obj * 1e-4;
       break; 
    end
end
end
```

# Reference
1. Bubeck, Sébastien. "Theory of convex optimization for machine learning." arXiv preprint arXiv:1405.4980 (2014).
2. [Revisiting Nesterov’s Acceleration](https://blogs.princeton.edu/imabandit/2015/06/30/revisiting-nesterovs-acceleration/)
3. Sutskever, Ilya, et al. "On the importance of initialization and momentum in deep learning." Proceedings of the 30th international conference on machine learning (ICML-13). 2013.
